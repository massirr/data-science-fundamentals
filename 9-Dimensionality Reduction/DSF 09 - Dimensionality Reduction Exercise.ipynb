{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddae46f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "low_memory=False\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import seaborn as sns; sns.set()\n",
    "from scipy import stats\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02491645",
   "metadata": {},
   "source": [
    "## 9.1 Introduction & Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26672dd4",
   "metadata": {},
   "source": [
    "We've seen quite a few regression, classification, and clustering methods. That's great! We can already train many different models and gain insights from them. However, now it's time to take a step back and look at the data we are feeding our models. Instead of just throwing everything we have at it, we are going to reduce our data by performing **Principal Component Analysis (PCA)**.\n",
    "\n",
    "Why do we do this? When working with large datasets, training a model becomes more complex and computationally expensive. For example, think of the training process of a GPT model. The more data being used, the more time it will take to train and the more CPU power it will consume, requiring expensive machinery and driving up electricity bills significantly. However, we still want to retain most of the information contained in our data. That's where PCA comes into play. We reduce data by transforming it into a smaller set of core components while preserving the most important information.\n",
    "\n",
    "**Key Benefits of PCA:**\n",
    "- Reduces computational complexity\n",
    "- Speeds up training time\n",
    "- Reduces storage requirements\n",
    "- Helps visualize high-dimensional data\n",
    "- Can reduce noise in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ac9157",
   "metadata": {},
   "source": [
    "## 9.2 Problem Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936885ff",
   "metadata": {},
   "source": [
    "The faces dataset is a useful dataset for exploring the differences between models and the effects of dimensionality reduction techniques. This makes it a prime candidate for exploring PCA!\n",
    "\n",
    "Similar to the digits dataset, it contains a collection of images and their corresponding labels. This time, however, the images are not handwritten digits but faces of famous American politicians. Let's explore the dataset and discover what we can achieve with PCA!\n",
    "\n",
    "**Dataset Overview:**\n",
    "- Contains facial images of politicians\n",
    "- Each image has multiple pixel values (features)\n",
    "- High-dimensional data perfect for demonstrating PCA\n",
    "- Real-world application of dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329ad8da",
   "metadata": {},
   "source": [
    "## 9.3 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451f682f",
   "metadata": {},
   "source": [
    "First, let's have a look at the data.\n",
    "\n",
    "**Hint:** Pay attention to the shape of the data. This will help you understand how many features (dimensions) we're working with and how PCA can reduce this complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d72d3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "faces = fetch_lfw_people(min_faces_per_person=60)\n",
    "print(faces.target_names)\n",
    "print(faces.images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d14b07f",
   "metadata": {},
   "source": [
    "##### Question 1: By now you should be familiar with the digits dataset. Visualize the faces dataset in a similar way. Show only the first 5 faces. Can you display the correct name as labels?\n",
    "\n",
    "**Hints:**\n",
    "- Use `plt.subplot()` to create multiple plots in one figure\n",
    "- The image data needs to be reshaped to display properly (check the image shape from above)\n",
    "- Use `target_names[label]` to get the actual person's name\n",
    "- Consider using a grayscale colormap for better visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed87803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3db1aeb",
   "metadata": {},
   "source": [
    "##### Question 2: Train a classification method of your choice to get some predictions. Use cross-validation to get the best results. Remember to select the best parameters!\n",
    "\n",
    "**Hints:**\n",
    "- KNN is a good choice for image classification\n",
    "- Try different values of k (number of neighbors) to find the optimal parameter\n",
    "- Use train-test split to evaluate different parameter values\n",
    "- Plot the accuracy vs. parameter values to visualize the best choice\n",
    "- This baseline performance will be important for comparing with PCA results later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7341e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9749e0",
   "metadata": {},
   "source": [
    "## 9.4 Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f08940",
   "metadata": {},
   "source": [
    "##### Question 3: It's time to reduce our data using PCA! Figure out the best amount of principal components and reduce the data.\n",
    "\n",
    "**Hints:**\n",
    "- Start by fitting PCA without specifying the number of components to see all eigenvalues\n",
    "- Plot the explained variance ratio (eigenvalues) to visualize the \"elbow\"\n",
    "- The elbow method helps identify where additional components provide diminishing returns\n",
    "- Remember: each eigenvalue represents how much variance that component explains\n",
    "- Look for the point where the slope significantly decreases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c725426e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe53f1a",
   "metadata": {},
   "source": [
    "##### Question 4: Do you notice something special about the eigenvalues? What is the total sum of all eigenvalues? Play around with models with different amounts of components. How does this change? Is this expected? Elaborate based on the meaning of eigenvalue.\n",
    "\n",
    "**Hints:**\n",
    "- Calculate the sum of explained variance ratios for different numbers of components\n",
    "- Think about what 100% variance means in the context of the original data\n",
    "- Remember: eigenvalues represent the proportion of total variance explained by each component\n",
    "- Consider what happens when you include ALL components vs. just a subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071f32d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a601d7",
   "metadata": {},
   "source": [
    "##### Question 5: Use PCA and the best amount of components you found earlier to reduce your data. Retrain your model using k-fold and compare the accuracy. What do you notice?\n",
    "\n",
    "**Steps to follow:**\n",
    "1. Apply PCA with your chosen number of components to transform your data\n",
    "2. Split the transformed data for training and testing\n",
    "3. Find the optimal k value for KNN on the reduced data\n",
    "4. Use k-fold cross-validation to get a reliable accuracy estimate\n",
    "5. Compare this accuracy with your baseline from Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd42a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3900db18",
   "metadata": {},
   "source": [
    "##### Question 6: Your department just got granted some extra budget. You are able to use some more processing power, but still not enough to use the entire dataset. Your boss wants you to create a model that retains 90% of all variance. Create this model and calculate the accuracy as before. By how much did you reduce the size of your dataset?\n",
    "\n",
    "**Hints:**\n",
    "- You can specify the variance ratio directly in PCA: `PCA(0.9)`\n",
    "- This will automatically determine how many components are needed to retain 90% of variance\n",
    "- Compare the number of components used with the original features\n",
    "- Calculate the percentage reduction: `(original_features - new_features) / original_features * 100`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ef9200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a98da7",
   "metadata": {},
   "source": [
    "## 9.5 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bab9e78",
   "metadata": {},
   "source": [
    "##### Question 1: See section 9.3\n",
    "##### Question 2: See section 9.3\n",
    "##### Question 3: See section 9.4\n",
    "##### Question 4: See section 9.4\n",
    "##### Question 5: See section 9.4\n",
    "##### Question 6: See section 9.4\n",
    "\n",
    "##### Question 7: PCA is used to reduce the number of variables by creating new variables that explain multiple previous ones. By definition, you should get somewhat usable results when reversing this process. Transform your projected data from Question 6 back into the original number of dimensions and compare the data by looking at the data points of the first face.\n",
    "\n",
    "**Hints:**\n",
    "- Use `pca.inverse_transform()` to reconstruct the original data\n",
    "- Compare the original `faces.data[0]` with the reconstructed version\n",
    "- The reconstructed data won't be identical but should be very similar\n",
    "- This process demonstrates that PCA preserves the most important information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa1e959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620d6904",
   "metadata": {},
   "source": [
    "##### Question 8: Now that you have figured out the process of reversing PCA, visualize the reconstructed data. Compare the reconstructed faces with the original ones and behold the true power of PCA!\n",
    "\n",
    "**Instructions:**\n",
    "- Create side-by-side visualizations: original faces vs. reconstructed faces\n",
    "- Use the same visualization code from Question 1\n",
    "- Apply PCA with 90% variance, then use inverse_transform\n",
    "- Compare how well the faces are preserved despite the massive dimensionality reduction\n",
    "\n",
    "**What to look for:**\n",
    "- Overall facial structure should be well preserved\n",
    "- Key facial features should remain recognizable\n",
    "- Some fine details might be slightly blurred (this is the 10% variance we discarded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db79019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for original faces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2f32a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for reconstructed faces\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6914318",
   "metadata": {},
   "source": [
    "##### Question 9: We talked briefly about how PCA can be used to reduce noise. Assume the 'noisy' data as seen below. Plot the faces as you did before to visually see the noise. What's the best result you can achieve when using PCA to reduce the noise?\n",
    "\n",
    "**Experiment Design:**\n",
    "1. First, visualize the noisy data to see the effect of added noise\n",
    "2. Apply PCA to the original clean data (not the noisy data)\n",
    "3. Use inverse_transform to reconstruct the data\n",
    "4. Compare: Original → Noisy → PCA-reconstructed\n",
    "\n",
    "**Key Insight:** PCA learns patterns from clean data and can filter out noise when reconstructing because noise typically has low variance and gets captured in the discarded components.\n",
    "\n",
    "**Try different variance thresholds:** Test 90%, 95%, 99% to see which gives the best noise reduction while preserving facial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb8bd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "noisy = np.random.normal(faces.data, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afcfc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here to visualize noisy data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853a47a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here to apply PCA noise reduction\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
