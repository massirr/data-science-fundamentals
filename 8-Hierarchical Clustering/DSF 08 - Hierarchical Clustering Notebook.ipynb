{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a1dd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_memory=False\n",
    "# Import required libraries\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0d9823",
   "metadata": {},
   "source": [
    "## 8.1 Introduction & Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cd6cdf",
   "metadata": {},
   "source": [
    "The k-Means algorithm is excellent at detecting clusters when we know beforehand exactly how many clusters we expect to find. However, as we've seen, it becomes more challenging when the number of clusters is unknown. We could calculate evaluation metrics (like we did with the elbow method), fit multiple k-Means models with different k values, and compare them all. Alternatively, we can use **Hierarchical Clustering**, which offers a more flexible approach.\n",
    "\n",
    "**Key Advantage:** Hierarchical clustering doesn't require us to specify the number of clusters upfront. Instead, it creates a hierarchy of clusters that we can cut at any level to get our desired number of groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fe8441",
   "metadata": {},
   "source": [
    "## 8.2 Problem Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78381ba",
   "metadata": {},
   "source": [
    "We're continuing to work with the unsupervised version of the digits dataset from the previous chapter. As a reminder:\n",
    "\n",
    "* **Dataset:** Handwritten digits (0-9)\n",
    "* **Features:** 64 variables representing pixel intensities in 8x8 images\n",
    "* **Observations:** 1,797 digit images\n",
    "* **Task:** Group similar digits together without using the true labels\n",
    "\n",
    "This allows us to compare how hierarchical clustering performs compared to k-Means on the same problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a16b700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the digits dataset\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa35f4aa",
   "metadata": {},
   "source": [
    "## 8.3 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0119aa99",
   "metadata": {},
   "source": [
    "### 8.3.1 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10217e7",
   "metadata": {},
   "source": [
    "The idea behind hierarchical clustering is remarkably simple and intuitive. Consider the following dataset:\n",
    "\n",
    "![](https://s3.amazonaws.com/stackabuse/media/hierarchical-clustering-python-scikit-learn-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c5f3a5",
   "metadata": {},
   "source": [
    "We can clearly see two clusters in this visualization, but we need an algorithm to identify them systematically. Here's how **agglomerative (bottom-up) hierarchical clustering** works:\n",
    "\n",
    "**Step-by-Step Process:**\n",
    "1. **Start:** Each observation begins as its own cluster (n clusters for n points)\n",
    "2. **Measure:** Calculate the distance between all pairs of clusters\n",
    "3. **Merge:** Combine the two closest clusters into one\n",
    "4. **Repeat:** Continue steps 2-3 until all points are in a single cluster\n",
    "\n",
    "**Result:** We create a hierarchy that contains solutions for every possible number of clusters (from n down to 1). This process can be visualized as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef384eaa",
   "metadata": {},
   "source": [
    "![](https://s3.amazonaws.com/stackabuse/media/hierarchical-clustering-python-scikit-learn-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeef808",
   "metadata": {},
   "source": [
    "The power of hierarchical clustering lies in its flexibility. We can \"cut\" the dendrogram (tree diagram) at any height to obtain our desired number of clusters. The horizontal line we draw across the tree determines how many clusters we end up with:\n",
    "\n",
    "* **Cut high:** Fewer, larger clusters\n",
    "* **Cut low:** More, smaller clusters\n",
    "\n",
    "This visual approach makes it easy to choose the optimal number of clusters:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84492023",
   "metadata": {},
   "source": [
    "![](https://s3.amazonaws.com/stackabuse/media/hierarchical-clustering-python-scikit-learn-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd191c6",
   "metadata": {},
   "source": [
    "### 8.3.2 Model Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d171d1f",
   "metadata": {},
   "source": [
    "Let's create a dendrogram to visualize the hierarchical structure of our digits dataset:\n",
    "\n",
    "**Note:** A dendrogram is a tree diagram that shows the arrangement of clusters. The height at which branches merge indicates the distance between clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2802b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dendrogram\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368c1da6",
   "metadata": {},
   "source": [
    "**Interpreting the Dendrogram:**\n",
    "\n",
    "Ideally, we draw our cutoff line where there's the largest vertical distance without any merges occurring. This represents a natural separation between clusters.\n",
    "\n",
    "One of the key advantages of hierarchical clustering is its flexibility: if certain digits are too similar and get confused, we can easily adjust by choosing a different number of clusters to improve separation.\n",
    "\n",
    "**For this example:** Since we know there are 10 distinct digits (0-9), we'll fit our model with 10 clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc6f525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit hierarchical clustering with 10 clusters\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fa3514",
   "metadata": {},
   "source": [
    "## 8.4 Exercises\n",
    "\n",
    "##### Question 1: Try to fit the elbow plot for the hierarchical clustering model on the digits dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8f0fd0",
   "metadata": {},
   "source": [
    "**Understanding the Metrics:**\n",
    "\n",
    "For k-Means, we used the distortion (inertia) metric to create elbow plots. However, hierarchical clustering doesn't use centroids, so distortion isn't applicable here.\n",
    "\n",
    "Instead, we use the **Linkage Criterion**, which represents the distance between clusters before they merge. The specific calculation depends on the linkage method:\n",
    "\n",
    "* **Ward linkage** (most common): Minimizes the variance within clusters\n",
    "* **Complete linkage**: Uses the maximum distance between cluster members\n",
    "* **Average linkage**: Uses the average distance between all pairs\n",
    "\n",
    "**Hint:** The Ward method is generally preferred because it creates compact, spherical clusters similar to k-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2990fa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create elbow plot using linkage criterion\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba2d8c0",
   "metadata": {},
   "source": [
    "**Analysis Questions:**\n",
    "\n",
    "* Where does the elbow occur in your plot?\n",
    "* Does this match our expectation of 10 clusters?\n",
    "* Would 9 clusters also be a reasonable choice? Why or why not?\n",
    "* Why does the graph \"wobble\" more than k-Means elbow plots?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18cad53",
   "metadata": {},
   "source": [
    "**Your analysis here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9003b3",
   "metadata": {},
   "source": [
    "##### Question 2: Try to split the dataset between test and train and check how accurate the best possible hierarchical clustering model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2346d4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data and evaluate hierarchical clustering\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa38637a",
   "metadata": {},
   "source": [
    "**Analysis Questions:**\n",
    "\n",
    "* What accuracy did you achieve?\n",
    "* How does this compare to k-Means (which achieved ~75%)?\n",
    "* Which digits are most commonly confused?\n",
    "* Why do you think hierarchical clustering performs the way it does on this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1c10ec",
   "metadata": {},
   "source": [
    "**Your analysis here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3474e0f",
   "metadata": {},
   "source": [
    "##### Question 3: Compare the hierarchical clustering and the k-means clustering algorithm for this dataset. Report which one provides the best fit. Is this what you would expect?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979ac72a",
   "metadata": {},
   "source": [
    "**Your comparative analysis here:**\n",
    "\n",
    "Consider:\n",
    "* Performance differences (accuracy)\n",
    "* Advantages of each method\n",
    "* When would you prefer hierarchical clustering?\n",
    "* When would you prefer k-Means?\n",
    "* Why might hierarchical clustering perform better or worse on this specific dataset?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
