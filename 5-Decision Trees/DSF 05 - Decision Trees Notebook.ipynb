{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca10cb9f",
   "metadata": {},
   "source": [
    "# Chapter 5: Decision Trees\n",
    "\n",
    "**Student Learning Version**  \n",
    "Work through this notebook to understand decision trees. The solution notebook will be provided after completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f72297",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_memory=False\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b177a5",
   "metadata": {},
   "source": [
    "## 5.1 Introduction & Motivation\n",
    "\n",
    "Welcome to the fascinating world of decision trees! In this chapter, we'll explore one of the most intuitive and interpretable machine learning algorithms available to data scientists today."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bdfb34",
   "metadata": {},
   "source": [
    "Now that we've gained a solid foundation in classification models, we'll advance our understanding by introducing one of the most intuitive and interpretable classifiers: **decision trees**.\n",
    "\n",
    "Decision trees mirror the way humans naturally make decisions by breaking down complex problems into a series of simple yes/no questions. This makes them particularly valuable for understanding how predictions are made, unlike \"black box\" algorithms where the decision-making process is opaque.\n",
    "\n",
    "Consider the following decision tree, which demonstrates how we can systematically classify animals into their respective families using a series of logical questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8600b9d",
   "metadata": {},
   "source": [
    "![](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/figures/05.08-decision-tree.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a47d197",
   "metadata": {},
   "source": [
    "This animal classification tree perfectly illustrates how decision trees work in practice. Each internal node (like \"Has fur?\") represents a decision based on a feature, each branch represents the outcome of that decision, and each leaf node represents the final classification.\n",
    "\n",
    "While we could manually construct such a tree using countless $if-else$ statements, the real power of decision trees lies in their ability to automatically learn these decision rules from data. Machine learning algorithms can analyze datasets and determine the optimal sequence of questions to ask, creating decision trees that can classify new, unseen examples with remarkable accuracy.\n",
    "\n",
    "**Key advantages of decision trees:**\n",
    "- **Interpretability**: The decision-making process is transparent and easy to explain\n",
    "- **No assumptions**: They don't require assumptions about data distribution\n",
    "- **Handles mixed data types**: Can work with both numerical and categorical features\n",
    "- **Feature selection**: Automatically identifies the most important features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5128531",
   "metadata": {},
   "source": [
    "## 5.2 Problem Setting: Handwritten Digit Recognition\n",
    "\n",
    "To demonstrate the power and versatility of decision trees, we'll tackle a classic machine learning challenge: recognizing handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3c2d14",
   "metadata": {},
   "source": [
    "For our Decision Tree Classifier implementation, we'll use the same digits dataset that we employed in our Logistic Regression chapter. This strategic choice serves multiple purposes:\n",
    "\n",
    "1. **Direct Comparison**: By using identical data, we can make fair, apples-to-apples comparisons between different algorithms\n",
    "2. **Consistent Evaluation**: We can apply the same metrics and evaluation criteria across different models\n",
    "3. **Understanding Trade-offs**: We'll discover when decision trees might be preferred over logistic regression and vice versa\n",
    "\n",
    "**Dataset Overview:**\n",
    "The digits dataset contains 8×8 pixel grayscale images of handwritten digits (0-9), where each pixel intensity is represented as a value between 0 and 16. This creates a 64-dimensional feature space that our decision tree will navigate to make classifications.\n",
    "\n",
    "If any of the following concepts seem unfamiliar, we recommend revisiting the previous chapter on Logistic Regression for a detailed explanation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fc5e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "dir(digits)\n",
    "# data\n",
    "# Print to show there are 1797 images (8 by 8 images for a dimensionality of 64)\n",
    "print(\"Image Data Shape\" , digits.data.shape)\n",
    "# Print to show there are 1797 labels (integers from 0–9)\n",
    "print(\"Label Data Shape\", digits.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c872ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,4))\n",
    "for index, (image, label) in enumerate(zip(digits.data[0:5], digits.target[0:5])):\n",
    " plt.subplot(1, 5, index + 1)\n",
    " plt.imshow(np.reshape(image, (8,8)), cmap=plt.cm.gray)\n",
    " plt.title('Training: %i\\n' % label, fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f940d2",
   "metadata": {},
   "source": [
    "## 5.3 Understanding Decision Trees: From Theory to Practice\n",
    "\n",
    "Decision trees work by recursively partitioning the feature space into regions that are as homogeneous as possible with respect to the target variable. Let's explore how this process works both theoretically and practically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb325f3",
   "metadata": {},
   "source": [
    "### 5.3.1 The Decision Tree Algorithm: How It Works\n",
    "\n",
    "Understanding decision trees conceptually is crucial before diving into implementation. While the mathematical foundations involve concepts like information gain, entropy, and Gini impurity, we'll focus on the intuitive understanding that makes decision trees so appealing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fee7a35",
   "metadata": {},
   "source": [
    "The concrete mathematical setup of decision tree classifiers involves sophisticated concepts from information theory. However, the beauty of decision trees lies in their intuitive nature, which we can understand through visual examples.\n",
    "\n",
    "**Core Concept**: Decision trees work by asking a series of questions about the features in your data, with each question designed to split the data into groups that are as \"pure\" as possible (containing mostly one class).\n",
    "\n",
    "To illustrate this process, let's examine a simple two-dimensional classification problem where we can visualize exactly how the algorithm makes its decisions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27f12f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=300, centers=4,\n",
    "                  random_state=0, cluster_std=1.0)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbd2704",
   "metadata": {},
   "source": [
    "**Understanding the Visualization:**\n",
    "- **X-axis and Y-axis**: These represent two independent variables (features) in our dataset\n",
    "- **Colors**: Each color represents a different class (category) that we want to predict\n",
    "- **Goal**: Create a decision tree that can accurately classify new points based on their X and Y coordinates\n",
    "\n",
    "This scatter plot shows four distinct clusters of data points, each representing a different class. A human can easily see the patterns, but how does a computer algorithm learn to distinguish between these groups?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49436ce1",
   "metadata": {},
   "source": [
    "**The Decision Tree Learning Process:**\n",
    "\n",
    "When a decision tree algorithm analyzes this data, it follows these steps:\n",
    "\n",
    "1. **Initial Assessment**: Start with all data points mixed together\n",
    "2. **Find the Best Split**: Identify the feature and threshold that best separates the classes\n",
    "3. **Create Branches**: Split the data into two groups based on this criterion\n",
    "4. **Repeat Recursively**: Apply the same process to each new group\n",
    "5. **Stop When Pure**: Continue until each group contains mostly one class or meets stopping criteria\n",
    "\n",
    "**Key Insight**: The algorithm draws lines (splits) through the feature space, with each line representing a decision boundary. These lines are always parallel to the axes because each split considers only one feature at a time.\n",
    "\n",
    "The following diagram illustrates how this iterative splitting process creates increasingly pure regions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42babf6d",
   "metadata": {},
   "source": [
    "![](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/figures/05.08-decision-tree-levels.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd05a33",
   "metadata": {},
   "source": [
    "### 5.3.2 Model Implementation and Training\n",
    "\n",
    "Now that we understand the theory behind decision trees, let's implement one using scikit-learn and apply it to our handwritten digits dataset. The implementation process follows the standard machine learning workflow we've established in previous chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc9d806",
   "metadata": {},
   "source": [
    "**Step 1: Data Preparation**\n",
    "\n",
    "Following machine learning best practices, we begin by splitting our dataset into training and testing portions. This separation is crucial for obtaining unbiased estimates of our model's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a91df3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45902ea5",
   "metadata": {},
   "source": [
    "**Step 2: Model Creation and Training**\n",
    "\n",
    "Similar to our Logistic Regression implementation, we instantiate a decision tree classifier and train it on our data. The scikit-learn implementation handles all the complex mathematics behind the scenes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9d4018",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "tree_fit = tree.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f28b859",
   "metadata": {},
   "source": [
    "**Step 3: Making Predictions**\n",
    "\n",
    "With our trained model, we can now make predictions on our test set. Each prediction represents the model's best guess about which digit (0-9) is represented in each test image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c132282a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tree.predict(x_test)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5ff806",
   "metadata": {},
   "source": [
    "## 5.4 Model Evaluation: Comprehensive Performance Analysis\n",
    "\n",
    "Evaluating our decision tree's performance requires examining multiple metrics to gain a complete understanding of how well our model performs. We'll use the same evaluation framework established in previous chapters to enable direct comparison with other algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7d3f95",
   "metadata": {},
   "source": [
    "### Accuracy: The Foundation Metric\n",
    "\n",
    "**What is Accuracy?**\n",
    "Accuracy measures the proportion of correct predictions out of all predictions made. It's calculated as:\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}$$\n",
    "\n",
    "This metric provides a quick, intuitive understanding of overall model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3735395a",
   "metadata": {},
   "source": [
    "Let's calculate our decision tree's accuracy and compare it with our previous logistic regression results. This comparison will help us understand the relative strengths and weaknesses of each approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4cbb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = tree_fit.score(x_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a94e6c",
   "metadata": {},
   "source": [
    "##### Question 1: Interpret the accuracy of this model. Based on this result, would you prefer the decision tree or the logistic regression classifier for this dataset? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79aa8469",
   "metadata": {},
   "source": [
    "**Your Analysis:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d99c26",
   "metadata": {},
   "source": [
    "### Precision, Recall, and F1-Score: Class-Level Performance\n",
    "\n",
    "While accuracy gives us an overall picture, precision, recall, and F1-score provide deeper insights into how well our model performs for each individual digit (0-9). These metrics are particularly important for multi-class problems like digit recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d41e2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b75a2c",
   "metadata": {},
   "source": [
    "##### Question 2: Analyze the precision, recall, and F1-scores for each digit class. Which digits does the decision tree classify most accurately, and which ones pose the greatest challenges? Compare these results with logistic regression performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeed6f0",
   "metadata": {},
   "source": [
    "**Your Detailed Analysis:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c77b5f",
   "metadata": {},
   "source": [
    "### Confusion Matrix: Visualizing Classification Patterns\n",
    "\n",
    "The confusion matrix provides a comprehensive view of our model's classification behavior, showing exactly which digits are being confused with others. This visualization is invaluable for understanding systematic errors in our model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64929fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_test, predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0a0360",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score: {0}'.format(score)\n",
    "plt.title(all_sample_title, size = 15);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b407ffcb",
   "metadata": {},
   "source": [
    "##### Question 3: Analyze the confusion matrix visualization above. What patterns do you observe in the misclassifications? How does this compare to the logistic regression confusion matrix, and what does this tell us about the nature of each algorithm's decision-making process?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ecc1ee",
   "metadata": {},
   "source": [
    "**Your Confusion Matrix Analysis:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9fa5c5",
   "metadata": {},
   "source": [
    "## 5.5 Exercises: Deepening Your Understanding\n",
    "\n",
    "These exercises will help consolidate your understanding of decision trees and develop your skills in comparative model analysis. Work through each question systematically, using both the theoretical concepts and practical results we've explored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a86546",
   "metadata": {},
   "source": [
    "##### Question 4: Comparative Confusion Matrix Analysis\n",
    "Comparing confusion matrices between different algorithms can be challenging due to their complexity. Create a comprehensive visual analysis that shows the differences in prediction patterns between Logistic Regression and Decision Tree classifiers. Your analysis should help identify where each algorithm excels or struggles.\n",
    "\n",
    "**Implementation Requirements:**\n",
    "1. Train both a Logistic Regression and Decision Tree model on the same dataset\n",
    "2. Generate confusion matrices for both models\n",
    "3. Create a visualization showing the difference between the matrices\n",
    "4. Interpret the results to understand algorithmic differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0dab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ab7534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your implementation here\n",
    "# Train both models, generate confusion matrices, and create comparative visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359dc8d2",
   "metadata": {},
   "source": [
    "**Your Comparative Analysis:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08198476",
   "metadata": {},
   "source": [
    "##### Question 5: Algorithm Comparison and Selection Guidelines\n",
    "\n",
    "Based on your research and the practical results from this chapter, provide a comprehensive comparison between logistic regression and decision trees. Your analysis should address:\n",
    "\n",
    "**Performance Characteristics:**\n",
    "- When does each algorithm typically excel?\n",
    "- What types of data favor each approach?\n",
    "- How do computational requirements compare?\n",
    "\n",
    "**Interpretability and Explainability:**\n",
    "- Which algorithm provides clearer insights into decision-making?\n",
    "- When might interpretability be more important than pure accuracy?\n",
    "\n",
    "**Practical Considerations:**\n",
    "- Data preprocessing requirements\n",
    "- Hyperparameter tuning complexity  \n",
    "- Scalability to large datasets\n",
    "\n",
    "**Recommendation Framework:**\n",
    "Develop guidelines for choosing between these algorithms based on project requirements, data characteristics, and business constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cddb1a",
   "metadata": {},
   "source": [
    "**Your Comprehensive Analysis:**\n",
    "\n",
    "**Logistic Regression Advantages:**\n",
    "\n",
    "**Decision Tree Advantages:**\n",
    "\n",
    "**When to Choose Logistic Regression:**\n",
    "\n",
    "**When to Choose Decision Trees:**\n",
    "\n",
    "**Recommendation Framework:**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
